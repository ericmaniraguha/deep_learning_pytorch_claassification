{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Simple multi-layer perceptron (MLP) for digit classification using the MNIST dataset in PyTorch"
      ],
      "metadata": {
        "id": "I9l0u0PFdUnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install torchvision - Providing Datasets, Transforms, and Image/Video Operations for PyTorch"
      ],
      "metadata": {
        "id": "1YiAdJREwIS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VC57qOY0bv2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5015a4b8-6c1d-49f9-cb87-3a8c3cee1448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import libraries"
      ],
      "metadata": {
        "id": "Kaxu1wb6w_RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "U0xQACBnd6_C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define instance of MLP model"
      ],
      "metadata": {
        "id": "e8A4j6fNxImA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_Model, self).__init__()\n",
        "        # Define the layers: input size, hidden layer, and output layer\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input size is 28x28, output size is 128\n",
        "        self.fc2 = nn.Linear(128, 64)       # Hidden layer with 64 units\n",
        "        self.fc3 = nn.Linear(64, 10)        # Output layer with 10 units (for digits 0-9)\n",
        "\n",
        "     # Forward function\n",
        "    def forward(self, x):\n",
        "      # Flatten the input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "      # Apply ReLU activation functions to hidden layers\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "      # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qKoN37pf5BKU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the training and testing datasets"
      ],
      "metadata": {
        "id": "guHUFiHe5-7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is loader data of MNIST dataset for training and testing.\n",
        "def load_data(batch_size=80):\n",
        "    # The Definition of data transformations for converting images to tensors.\n",
        "    transform_dataset_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    # Load training dataset\n",
        "    training_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_dataset_to_tensor)\n",
        "    load_training_dataset = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load testing dataset\n",
        "    testing_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_dataset_to_tensor)\n",
        "    load_testing_dataset = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    #return dataset trained and tested\n",
        "    return load_training_dataset, load_testing_dataset"
      ],
      "metadata": {
        "id": "wpf_F8WZ5H3x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to display a sample of images from a data loader"
      ],
      "metadata": {
        "id": "k2SUy8njLITl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def display_sample_images(data_loader, num_samples=5):\n",
        "    data_iter = iter(data_loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 3))\n",
        "    for i in range(num_samples):\n",
        "        ax = axes[i]\n",
        "        image = images[i].squeeze().numpy()\n",
        "        label = labels[i].item()\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(f\"Label: {label}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lkiBlRCtI6yZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model on the training dataset"
      ],
      "metadata": {
        "id": "YYJevojt6Jnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function has purpose to train the provided neural network model using\n",
        "# mini-batch gradient descent with the specified number of epochs and learning rate.\n",
        "def train_model(model, load_training_dataset, n_epochs=5, learning_rate=0.001):\n",
        "    # Define loss function (cross-entropy) and optimizer (Adam)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # The function importance lies in its role in training, evaluating, and saving a neural network model for digit classification.\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(load_training_dataset, 0):\n",
        "            inputs, labels = data  # Get the inputs and labels from the data loader\n",
        "            optimizer.zero_grad()  # Zero the gradient buffers of all model parameters\n",
        "            outputs = model(inputs)  # compute model predictions\n",
        "            loss = criterion(outputs, labels)  # Compute the loss between predictions and labels\n",
        "            loss.backward()  # Backpropagation: compute gradients\n",
        "            optimizer.step()  # Update model parameters using the computed gradients\n",
        "            running_loss += loss.item()  # Track the running loss\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "    # Print the training progress by displaying epoch, batch, and loss of\n"
      ],
      "metadata": {
        "id": "TpD1Wyxw5OhZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate the trained model on the testing dataset"
      ],
      "metadata": {
        "id": "j93FEV-W6SpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, load_testing_dataset):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in load_testing_dataset:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "# calculate accuracy of my model\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"\\n Accuracy on the test dataset: {accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "LPl4ODtSd2H2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optionally, save the trained model to a file"
      ],
      "metadata": {
        "id": "0fe9sCwQ6X_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the trained model to mnist_mlp_trained_model.pth\n",
        "def save_model(model, filename='mnist_mlp_trained_model.pth'):\n",
        "    torch.save(model.state_dict(), filename)"
      ],
      "metadata": {
        "id": "oZ_Do5se5dbh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call all functions"
      ],
      "metadata": {
        "id": "m3V455XwwBX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your MLP model\n",
        "model = MLP_Model()\n",
        "\n",
        "# Load the training and testing datasets\n",
        "load_training_dataset, load_testing_dataset = load_data(batch_size=80)\n",
        "\n",
        "# Display a sample of training images\n",
        "display_sample_images(load_training_dataset, num_samples=5)\n",
        "\n",
        "# Train the model on the training dataset\n",
        "train_model(model, load_training_dataset, n_epochs=5, learning_rate=0.001)\n",
        "\n",
        "# Evaluate the trained model on the testing dataset\n",
        "evaluate_model(model, load_testing_dataset)\n",
        "\n",
        "# Save the trained model to a file - mnist_mlp_trained_model.pth\n",
        "save_model(model, filename='mnist_mlp_trained_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9FPld1_kveS9",
        "outputId": "d38faa7d-0c4b-4dd8-9646-7dfca9ee61b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 425400952.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 34335514.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 173421886.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15289348.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAADKCAYAAACR8ty/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdX0lEQVR4nO3de3RNZxrH8edIlBBLXBJmgpQWqRmrnaEkpKW0jSoRRDKzik4npVMMJWqw6jILLUViiFFmaOs2LmliFEOn05TqJDHqMoK4hCyXDpNQl7TIbc8fs5pF97s5Jzkn57znfD9r+cMv79n7SbzbzmPLc2yGYRgCAAAAAICmarm7AAAAAAAAqoPGFgAAAACgNRpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GtgoKCgrEZrPJggULnHbMzz//XGw2m3z++edOOybgCux/+DquAfgy9j98HdeA5/KZxvaDDz4Qm80m+/fvd3cpLnHixAkZP368dOvWTerWrSs2m00KCgrcXRY8hLfv//T0dElISJA2bdpIvXr1pH379pKUlCTXrl1zd2nwEN5+DYiIfPrpp/LMM89I06ZNJSgoSLp06SJr1qxxd1nwAL6w/+/23HPPic1mkzFjxri7FHgIX7gGNmzYID//+c+lbt26EhwcLImJiVJUVOTusmqUzzS23i4rK0sWL14sN2/elMcee8zd5QA1auTIkXL8+HEZOnSoLF68WPr06SOpqakSGRkpt27dcnd5gMtt3bpVnn/+eSkpKZGZM2fKnDlzJCAgQIYPHy4pKSnuLg+oMenp6ZKVleXuMoAatWzZMvnlL38pjRs3luTkZBkxYoRs2LBBevfuLbdv33Z3eTXG390FwDliYmLk2rVr0qBBA1mwYIEcOnTI3SUBNSYtLU169ux5T9apUyd5+eWXZd26dfLqq6+6pzCghqSmpsqPfvQj+eyzz6ROnToiIvLaa69JeHi4fPDBBzJ+/Hg3Vwi43u3btyUpKUl+97vfyfTp091dDlAjSkpKZOrUqfL000/L3//+d7HZbCIi0q1bN+nfv7/86U9/kt/+9rdurrJm8MT2LiUlJTJ9+nTp1KmTNGzYUOrXry9PPfWUZGZmWr4mJSVFwsLCJCAgQHr06CG5ubmmNXl5eRIXFyeNGzeWunXrSufOnWXr1q0PrOe7776TvLw8u/4bQePGjaVBgwYPXAdY0Xn//7CpFREZOHCgiIgcP378ga8HRPS+Bm7cuCGNGjWqbGpFRPz9/aVp06YSEBDwwNcDOu//77377rtSUVEhEydOtPs1wPd0vQZyc3Pl2rVrkpCQUNnUioj069dPAgMDZcOGDQ88l7egsb3LjRs35M9//rP07NlT5s2bJzNnzpTCwkKJjo5WPgFdvXq1LF68WEaPHi1TpkyR3Nxc6dWrl1y+fLlyzdGjRyUiIkKOHz8ukydPloULF0r9+vUlNjZWMjIy7lvPvn375LHHHpPU1FRnf6qAibft/0uXLomISNOmTav0evgena+Bnj17ytGjR2XatGly+vRpyc/Pl1mzZsn+/ftl0qRJDn8t4Ht03v8iIufOnZO5c+fKvHnz+MccVImu18CdO3dERJT7PiAgQA4ePCgVFRV2fAW8gOEj3n//fUNEjH/961+Wa8rKyow7d+7ck33zzTdGs2bNjF//+teV2dmzZw0RMQICAowLFy5U5jk5OYaIGOPHj6/MevfubXTs2NG4fft2ZVZRUWF069bNaNu2bWWWmZlpiIiRmZlpymbMmOHQ5zp//nxDRIyzZ8869Dp4L1/a/99LTEw0/Pz8jJMnT1bp9fAu3n4NFBcXG/Hx8YbNZjNExBARo169esaWLVse+Fp4P2/f/4ZhGHFxcUa3bt0qfy8ixujRo+16LbyfN18DhYWFhs1mMxITE+/J8/LyKu8HRUVF9z2Gt+CJ7V38/PzkoYceEhGRiooKuXr1qpSVlUnnzp3lwIEDpvWxsbESGhpa+fsuXbpI165dZceOHSIicvXqVfnss88kPj5ebt68KUVFRVJUVCRXrlyR6OhoOXXqlFy8eNGynp49e4phGDJz5kznfqKAgjft//Xr18vKlSslKSlJ2rZt6/Dr4Zt0vgbq1Kkj7dq1k7i4OPnLX/4ia9eulc6dO8vQoUMlOzvbwa8EfJHO+z8zM1M++ugjWbRokWOfNHAXXa+Bpk2bSnx8vHz44YeycOFCOXPmjHzxxReSkJAgtWvXFhHxmUGaDI/6ge83RV5enpSWllbmrVu3Nq1VfcPcrl072bRpk4iInD59WgzDkGnTpsm0adOU5/vvf/97z0UBuJM37P8vvvhCEhMTJTo6WubMmePUY8P76XoNjBkzRrKzs+XAgQNSq9b//806Pj5efvKTn8i4ceMkJyen2ueA99Nx/5eVlcnYsWNl2LBh8uSTT1brWICO14CIyPLly+XWrVsyceLEyp8xHzp0qDzyyCOSnp4ugYGB1T6HDmhs77J27Vr51a9+JbGxsfLmm29KSEiI+Pn5yTvvvCP5+fkOH+/7/88+ceJEiY6OVq559NFHq1Uz4CzesP8PHz4sMTEx8tOf/lTS0tLE35+/4mA/Xa+BkpISWblypUyaNKmyqRURqV27trzwwguSmpoqJSUllU8iABVd9//q1avlxIkTsnz5cikoKLjnYzdv3pSCggIJCQmRevXqVftc8G66XgMiIg0bNpS//vWvcu7cOSkoKJCwsDAJCwuTbt26SXBwsAQFBTnlPJ6O7/rukpaWJm3atJH09PR7porNmDFDuf7UqVOm7OTJk/Lwww+LiEibNm1E5P/fXDz77LPOLxhwIt33f35+vvTp00dCQkJkx44dPvOvk3AeXa+BK1euSFlZmZSXl5s+VlpaKhUVFcqPAXfTdf+fO3dOSktLpXv37qaPrV69WlavXi0ZGRkSGxvrshrgHXS9Bu7WqlUradWqlYiIXLt2Tb766isZPHhwjZzbE/Aztnfx8/MTERHDMCqznJwcyzf63rJlyz3/N37fvn2Sk5MjL7zwgoiIhISESM+ePWX58uXyn//8x/T6wsLC+9ZTlVH3QFXpvP8vXbokzz//vNSqVUt27dolwcHBD3wN8EO6XgMhISESFBQkGRkZUlJSUpkXFxfLxx9/LOHh4UyJxQPpuv9/8YtfSEZGhumXiEjfvn0lIyNDunbtet9jACL6XgNWpkyZImVlZT71PuY+98R21apVsnPnTlM+btw46devn6Snp8vAgQPlxRdflLNnz8p7770nHTp0kOLiYtNrHn30UYmKipLXX39d7ty5I4sWLZImTZrc89YKS5culaioKOnYsaOMGDFC2rRpI5cvX5asrCy5cOGCHD582LLWffv2yTPPPCMzZsx44A+OX79+XZYsWSIiIl9++aWIiKSmpkpQUJAEBQXJmDFj7PnywMt56/7v06ePnDlzRiZNmiR79+6VvXv3Vn6sWbNm8txzz9nx1YEv8MZrwM/PTyZOnChvvfWWREREyPDhw6W8vFxWrlwpFy5ckLVr1zr2RYLX8sb9Hx4eLuHh4cqPtW7dmie1uIc3XgMiInPnzpXc3Fzp2rWr+Pv7y5YtW+STTz6R2bNn+9bPnrthErNbfD/m2+rX+fPnjYqKCuPtt982wsLCjDp16hg/+9nPjG3bthkvv/yyERYWVnms78d8z58/31i4cKHRsmVLo06dOsZTTz1lHD582HTu/Px8Y/jw4Ubz5s2N2rVrG6GhoUa/fv2MtLS0yjXVHXX/fU2qX3fXDt/k7fv/fp9bjx49qvGVg7fw9mvAMAxj3bp1RpcuXYygoCAjICDA6Nq16z3ngO/yhf3/Q8Lb/eAu3n4NbNu2zejSpYvRoEEDo169ekZERISxadOm6nzJtGQzjLuetwMAAAAAoBl+xhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABa87d3oc1mc2UdwAO58y2X2f9wN3e/5TjXANyNewB8GfcA+Dp7rgGe2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBr/u4uAIBni4iIUObJycnKPDIy0pRlZWUp106YMEGZZ2dn21kdAAAAwBNbAAAAAIDmaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWbIZhGHYttNlcXQtwX3ZuVZfwhf3fsmVLZf7ll186tN4ZEhISlPmmTZtcdk5P5879L+Ib1wA8G/cA+DLuAfB19lwDPLEFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGjN390FoPoWLVqkzMeOHavMZ8+ercynT5/urJKgocjISGVuNf148+bNyjwtLc2ULViwwKFjv/HGG8rcl6ciw/W6d++uzMeNG6fMo6KilHnz5s3tPuelS5eU+fr165X5lClTTFlpaand5wMAwFvxxBYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGjNZhiGYddCm83VtcAO4eHhpiwzM1O5NiQkRJnv27dPmVsND/IUdm5Vl/CF/W81yCkuLk6Zp6SkVPvY586ds/sYIuo9mp2d7dAxdOXO/S/ifdfAgAEDTNnatWuVa2/cuKHMjxw5osxPnTpldx0NGjRQ5oMGDVLmf/zjH02Z1eC/kpISu+vQAfcA3xEcHKzMd+/ebcrat2+vXGt1XcyZM6fqhbkR9wD4OnuuAZ7YAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArdHYAgAAAAC05u/uAuCY1157zZRZTT+2Mm/ePGeVAy9y/vx5Ze7I9GNHj52cnKzMJ0yYoMx9eSoyqqZ27drK/Pe//70py8vLU67t27evMi8sLKx6YQ9w8eJFZT558mRTlpOTo1ybkZHh1JqAmjJw4EBlrpqAbDUpVXWtiIh89NFHytzq+oce/P3VLc2wYcOU+apVq0yZ1V6ymghttX7dunWm7OzZs8q1r7zyijIPDQ1V5vaeT0Rk+PDhytzdE7ZdiSe2AAAAAACt0dgCAAAAALRGYwsAAAAA0BqNLQAAAABAazS2AAAAAACtMRXZQ7300kvKfNSoUXYfw2pi5+nTp6tUE+BsVtNcraimIjtjajO8V0JCgjLv2LGjKevTp49yrSunH1uZNWuWMu/Vq5cpGzRokHItU5H11rlzZ1O2fft25drg4GC7jyEicuDAgaoX5kZW02lVAgMDlbnVdc5UZD3Ur19fme/Zs0eZP/HEE8q8oqLC7nPeunVLmZeVlSnzmJgYU/bQQw8p15aUlCjz8vJyZV6rlvmZZHx8vHLt1KlTlbnVO1V4A57YAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArdHYAgAAAAC0xlRkD2U1yczf3/4/MquJmLm5uVWqCXC2rKwsh9a3aNHCRZXAWzVo0ECZFxUVmbIjR464uhy7WU3KvHz5sil79tlnlWutpsIWFxdXvTDUGNUE5CZNmijXGobh6nI8gurztPrcrabeHjt2zKk1oWY1atRImTs6/Tg/P9+ULVu2TLl227Ztdh/DSteuXZW51btDvPPOO8p80qRJpmzVqlXKtd48/dgKT2wBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWGB7lZp9++qkyb9u2rd3HmD59ujJfsmRJlWoCaoqjgw0iIyNdVAm81eDBg5X5+++/b8ouXbrk6nLs1qtXL2Xev39/U2Z1D2BIlN7y8vJMWVRUlHKtzWZT5laDKOPi4qpemBN16tRJmc+aNUuZW32eKlb3lwMHDth9DHiepKQkh9avWLFCmY8ePdoZ5djNakiUlT179ijzCRMmmLKCggLlWj8/P2VeXl7uUC064YktAAAAAEBrNLYAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrTEWuISEhIcq8d+/eyryiokKZX79+3ZQdOnRIufbGjRv2FQe4SUREhEPrHZ2iDFhNF96/f38NV6LWpEkTZZ6enq7Mv/rqK1M2f/58p9YEz/D222+bsu3btzt0jNjYWGUeHh5uylRTmF1txIgRytzqujAMw65MRKSwsFCZFxUV2Vkd3K1u3bqmzOr7Ziu6TsH+29/+psxPnjxpylR/V4hYT8ZfunRp1QvzcDyxBQAAAABojcYWAAAAAKA1GlsAAAAAgNZobAEAAAAAWqOxBQAAAABojanILhAUFGTK3nrrLeVaq2l+VtasWWPKHJ2SCHiKyMhIh9Zv3rzZRZXAW1lNluzevbspCwwMVK61mizpCKspr1ZT7a0MGjTIlJWUlFSlJHi4K1eumDKbzaZca5VbGTx4sCmbM2eOQ8dwxMCBA5X5yJEjlbnV90aOfJ5MP9afn5+fKQsNDXXoGKq9LqL+fsLb3k3k8ccfd3cJNY4ntgAAAAAArdHYAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArTEV2QVmzZplykaNGuXQMfbs2aPMp06dWqWagAeJiIhQ5q1atXLoOFlZWabs/PnzyrVDhgxx6Ng5OTkOrQeSk5OV+Y4dO0yZau+KiJw+fVqZW01ojYqKMmWNGze2KtGhYx87dsyUjRgxQrl248aNDp0TnqWwsNCuTEQkODhYmVtNF548ebIpa9++vXLt3r17rUq0uxbV+USs63PkHSOs1rpyyjNqxrfffmvKFixYoFw7e/ZsZR4dHa3MExMTTVlKSooD1cET8cQWAAAAAKA1GlsAAAAAgNZobAEAAAAAWqOxBQAAAABozWbY+RP6VoMsYLZkyRJTZjU8yurrGhMTo8y3bdtW9cI058gwCWfztv2/adMmU+boICdXsho25eggK2/izv0v4n3XwLBhw0yZ1bCZwMBAZR4UFKTMHfmzOnTokDJfvHixMk9NTTVl5eXlyrVdu3ZV5idOnLCvOA/DPUAkPDxcmauGiolYf81Un48ja3U4tp+fnzLXFfeA/2vYsKEy3717tzLv2LGjMlcNprL6Xt3qe+9r164pc1c6cuSIKevQoYNy7cqVK5X5yJEjnVpTTbHnGuCJLQAAAABAazS2AAAAAACt0dgCAAAAALRGYwsAAAAA0BqNLQAAAABAa0xFrobQ0FBlbjXRVWXu3LnKfM+ePcp8586ddh/b2zAR03ERERHKPCsrq4YrcY6EhARTpprw7I2YiOl6VtOPrfIf//jHynzXrl2m7MaNG8q1VtdoYWGhMn/jjTdM2cKFC5Vrt27dqswHDhyozD0d9wBr0dHRyvzDDz9U5sHBwabM26Yi+/v7K3NdcQ+4v7p16ypz1SR5EZFXXnnF7mMfOHBAmf/hD39Q5h9//LEpu379unJto0aNlPmAAQPsPqfVPYqpyAAAAAAAaIbGFgAAAACgNRpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI2pyHZo3LixMs/IyFDmUVFRdh+7Xr16yvzOnTt2H8NXMBHTcfHx8cp848aNpsxqmvfEiROVudVU8MjISFNmNfm1ZcuWytwRVnVnZ2c7tD4pKanatbgSEzE9z5tvvqnMVdPuY2JilGu3b9/u0DnDw8NN2dGjR5VrmYrsPN62/9esWWPKYmNjlWvr16+vzK3+PFR/x1q900NYWJgyt/o+ypGpyH5+fspcV9wDqiYgIECZ9+/f35QtXrxYuVY1Rfx+Dh48aMquXr2qXNu0aVNl/vjjjzt0ThWr+0tcXJwyLykpqfY5XYmpyAAAAAAAr0djCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtObv7gJ0EB0drcwdmX5shenH8BRWU4Q3bdrk0HFSUlJM2fjx45Vrk5OTHTq2itVkZUcnLm/evNmUWX1NABGR7t27K/Ovv/7alO3evdsp57x06ZIpu3z5slOODd8xbNgwU6aauC1i/e4NVs6dO2fKioqKlGvfe+89ZW51bamkp6fbvRa+59atW8pc9b3Nv//9b+XaUaNGKfMuXboo8yeffNLO6lzrxRdfVOZW7/aiur/ohie2AAAAAACt0dgCAAAAALRGYwsAAAAA0BqNLQAAAABAawyPssOQIUMcWv/tt9+aMk/5QXLASkREhMuObTU86vz588rcanBIXFxctWvJyspS5gyKgqMCAwOV+cGDB01ZcXGxU87ZvHlzU9asWTOnHBu+LS8vr8bP+fTTTytzm81md241mApwlNU1MHbsWGXeoEEDZd6hQwen1fRD69atM2WtW7d22fl0wxNbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWmIp8lwEDBijzHj16OHScJUuWmLITJ05UqSagOqwmAKu0bNlSmS9cuFCZJyUlKfP4+Hi7j71582ZlbjUtOSUlRZkD7tCrVy9l/u6779ZwJWpXr151dwnAfR07dkyZt2vXroYrARx38+ZNZZ6Tk+Oyc966dctlx/YGPLEFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGiNqch3GTNmjDIPCgpS5kVFRcp82bJlzioJqBar6cITJkwwZcnJyXavFRGJjIxU5i1atLCzOutzAp7kiSeecGj9zp07XVOIiPTp08futWlpaS6rA3CGQYMGKXPDMJS5zWZzZTmAxzt69Kgp69Chgxsq8Uw8sQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI2pyNVw4cIFh3LAU6SkpNi91mpysdVUZJWsrCxlnp2dbfcxAHcJCwtT5lYTWktLS11WyyOPPGLK8vLylGv/8Y9/uKwOwBmsph9b5SpW+x/wRlu2bDFlQ4YMqflCPBRPbAEAAAAAWqOxBQAAAABojcYWAAAAAKA1GlsAAAAAgNZ8dnhURESEKevQoYNy7ddff63M+WFteBOrgVIXL15U5hs3blTmqmFTSUlJVS8McLPi4mJlbjXgpn79+tU+Z4sWLZT5sGHDTNk///lP5dqSkpJq1wG4ktUANkfWh4eHO6scAJrjiS0AAAAAQGs0tgAAAAAArdHYAgAAAAC0RmMLAAAAANAajS0AAAAAQGs2w2qs4w8XOji5ztPl5OSYss6dOyvXfvPNN8q8b9++ynzfvn1VLwyW7NyqLuFt+x/6cef+F+EaUDly5IgyDwwMNGW/+c1vlGutJi4vWbJEmbdt29aUxcTEKNdmZmYqc11xD/A+5eXlytzqz1r157BixQrl2tdff73qhXkg7gEQEQkICDBlubm5yrUPP/ywMg8NDVXmly5dqnJdNcGea4AntgAAAAAArdHYAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArfm7uwB32bVrlylr3ry5cm1+fr4y/+6775xaEwBAH0uXLrU737Fjh0PHLi0tVeZz5swxZd42/Ri+48qVK8q8SZMmylz1fdcnn3zi1JoAT3br1i1TVlZW5tAxjh49qsxfeuklU7Zz506Hju1uPLEFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGjNZhiGYddCm83VtQD3ZedWdQn2P9zNnftfhGsA7sc9wPtER0cr81dffVWZr1+/3pRlZGQ4tSZPxT0AVhITE5X5ihUrHDrOokWLTFlSUlJVSnIJe64BntgCAAAAALRGYwsAAAAA0BqNLQAAAABAazS2AAAAAACtMTwK2mBwCHwZg0Pg67gHwJdxD4CVWrXUzynPnDmjzFu2bKnM4+LiTJknDWdjeBQAAAAAwOvR2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK0xFRnaYCImfBkTMeHruAfAl3EPgK9jKjIAAAAAwOvR2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3ZPRUZAAAAAABPxBNbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDW/gd1uEoMyY7EAQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.9864\n",
            "Epoch 1, Batch 200, Loss: 0.3679\n",
            "Epoch 1, Batch 300, Loss: 0.3203\n",
            "Epoch 1, Batch 400, Loss: 0.2867\n",
            "Epoch 1, Batch 500, Loss: 0.2599\n",
            "Epoch 1, Batch 600, Loss: 0.2308\n",
            "Epoch 1, Batch 700, Loss: 0.2164\n",
            "Epoch 2, Batch 100, Loss: 0.1759\n",
            "Epoch 2, Batch 200, Loss: 0.1742\n",
            "Epoch 2, Batch 300, Loss: 0.1682\n",
            "Epoch 2, Batch 400, Loss: 0.1528\n",
            "Epoch 2, Batch 500, Loss: 0.1423\n",
            "Epoch 2, Batch 600, Loss: 0.1407\n",
            "Epoch 2, Batch 700, Loss: 0.1292\n",
            "Epoch 3, Batch 100, Loss: 0.1081\n",
            "Epoch 3, Batch 200, Loss: 0.1050\n",
            "Epoch 3, Batch 300, Loss: 0.1032\n",
            "Epoch 3, Batch 400, Loss: 0.1217\n",
            "Epoch 3, Batch 500, Loss: 0.0968\n",
            "Epoch 3, Batch 600, Loss: 0.0976\n",
            "Epoch 3, Batch 700, Loss: 0.1039\n",
            "Epoch 4, Batch 100, Loss: 0.0794\n",
            "Epoch 4, Batch 200, Loss: 0.0816\n",
            "Epoch 4, Batch 300, Loss: 0.0800\n",
            "Epoch 4, Batch 400, Loss: 0.0843\n",
            "Epoch 4, Batch 500, Loss: 0.0859\n",
            "Epoch 4, Batch 600, Loss: 0.0720\n",
            "Epoch 4, Batch 700, Loss: 0.0771\n",
            "Epoch 5, Batch 100, Loss: 0.0584\n",
            "Epoch 5, Batch 200, Loss: 0.0609\n",
            "Epoch 5, Batch 300, Loss: 0.0708\n",
            "Epoch 5, Batch 400, Loss: 0.0621\n",
            "Epoch 5, Batch 500, Loss: 0.0632\n",
            "Epoch 5, Batch 600, Loss: 0.0662\n",
            "Epoch 5, Batch 700, Loss: 0.0611\n",
            "\n",
            " Accuracy on the test dataset: 97.040%\n"
          ]
        }
      ]
    }
  ]
}